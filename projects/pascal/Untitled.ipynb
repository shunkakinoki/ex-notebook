{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/notebooks/\")\n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.core import *\n",
    "from fastai.transforms import *\n",
    "\n",
    "\n",
    "model_meta = {\n",
    "\tresnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
    "\tvgg16:[0,22], vgg19:[0,22],\n",
    "\tresnext50: [8, 6], resnext101: [8, 6], resnext101_64: [8, 6],\n",
    "\tdn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
    "}\n",
    "\n",
    "model_features = {}  # {inception_4: 3072, dn121: 2048, dn161: 4416, nasnetalarge: 4032*2}\n",
    "\n",
    "\n",
    "class ConvnetBuilder(object):\n",
    "\t\"\"\"Class representing a convolutional network.\n",
    "\n",
    "\tArguments:\n",
    "\t\t\tf: a model creation function (e.g. resnet34, vgg16, etc)\n",
    "\t\t\tc (int): size of the last layer\n",
    "\t\t\tis_multi (bool): is multilabel classification?\n",
    "\t\t\t\t\t(def here http://scikit-learn.org/stable/modules/multiclass.html)\n",
    "\t\t\tis_reg (bool): is a regression?\n",
    "\t\t\tps (float or array of float): dropout parameters\n",
    "\t\t\txtra_fc (list of ints): list of hidden layers with # hidden neurons\n",
    "\t\t\txtra_cut (int): # layers earlier than default to cut the model, default is 0\n",
    "\t\t\tcustom_head : add custom model classes that are inherited from nn.modules at the end of the model\n",
    "\t\t\t\t\t\t\t\t\t\tthat is mentioned on Argument 'f'\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, pretrained=True):\n",
    "\t\tself.f, self.c, self.is_multi, self.is_reg, self.xtra_cut = f, c, is_multi, is_reg, xtra_cut\n",
    "\t\tif xtra_fc is None: xtra_fc = [512]\n",
    "\t\tif ps is None: ps = [0.25]*len(xtra_fc)+[0.5]\n",
    "\t\tself.ps, self.xtra_fc = ps, xtra_fc\n",
    "\n",
    "\t\tif f in model_meta:\n",
    "\t\t\tcut, self.lr_cut = model_meta[f]\n",
    "\t\telse:\n",
    "\t\t\tcut, self.lr_cut = 0, 0\n",
    "\t\tcut -= xtra_cut\n",
    "\t\tlayers = cut_model(f(pretrained), cut)\n",
    "\t\tself.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "\t\tif not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "\t\tself.top_model = nn.Sequential(*layers)\n",
    "\n",
    "\t\tn_fc = len(self.xtra_fc)+1\n",
    "\t\tif not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "\t\tif custom_head:\n",
    "\t\t\tfc_layers = [custom_head]\n",
    "\t\telse:\n",
    "\t\t\tfc_layers = self.get_fc_layers()\n",
    "\t\tself.n_fc = len(fc_layers)\n",
    "\t\tself.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "\t\tif not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "\t\tself.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "\t@property\n",
    "\tdef name(self):\n",
    "\t\treturn f'{self.f.__name__}_{self.xtra_cut}'\n",
    "\n",
    "\tdef create_fc_layer(self, ni, nf, p, actn=None):\n",
    "\t\tres = [nn.BatchNorm1d(num_features=ni)]\n",
    "\t\tif p: res.append(nn.Dropout(p=p))\n",
    "\t\tres.append(nn.Linear(in_features=ni, out_features=nf))\n",
    "\t\tif actn: res.append(actn)\n",
    "\t\treturn res\n",
    "\n",
    "\tdef get_fc_layers(self):\n",
    "\t\tres = []\n",
    "\t\tni = self.nf\n",
    "\t\tfor i, nf in enumerate(self.xtra_fc):\n",
    "\t\t\tres += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n",
    "\t\t\tni = nf\n",
    "\t\tfinal_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()\n",
    "\t\tif self.is_reg: final_actn = None\n",
    "\t\tres += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n",
    "\t\treturn res\n",
    "\n",
    "\tdef get_layer_groups(self, do_fc=False):\n",
    "\t\tif do_fc:\n",
    "\t\t\treturn [self.fc_model]\n",
    "\t\tidxs = [self.lr_cut]\n",
    "\t\tc = children(self.top_model)\n",
    "\t\tif len(c) == 3: c = children(c[0])+c[1:]\n",
    "\t\tlgs = list(split_by_idxs(c, idxs))\n",
    "\t\treturn lgs+[self.fc_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_reg4 = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(25088,256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256,4+20),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(arch=resnet34, **kwargs):\n",
    "\topts = dict(is_multi=False, is_reg=False, pretrained=False)\n",
    "\tconv = ConvnetBuilder(arch, 0, **opts, **kwargs)\n",
    "\treturn conv.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import cv2\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def download_file(bucket_name, object_key_name, file_path):\n",
    "\t\"\"\"  Downloads a file from an S3 bucket.\n",
    "\t:param bucket_name: S3 bucket name\n",
    "\t:param object_key_name: S3 object key name\n",
    "\t:param file_path: path to save downloaded file\n",
    "\t\"\"\"\n",
    "\ts3_client.download_file(bucket_name, object_key_name, file_path)\n",
    "\n",
    "\n",
    "def get_labels(path):\n",
    "\t\"\"\"  Get labels from a text file.\n",
    "\t:param path: path to text file\n",
    "\t:return: list of labels\n",
    "\t\"\"\"\n",
    "\twith open(path, encoding='utf-8', errors='ignore') as f:\n",
    "\t\tlabels = [line.strip() for line in f.readlines()]\n",
    "\t\tf.close()\n",
    "\treturn labels\n",
    "\n",
    "\n",
    "def open_image_url(url):\n",
    "\t\"\"\"  Opens an image using OpenCV from a URL.\n",
    "\t:param url: url path of the image\n",
    "\t:return: the image in RGB format as numpy array of floats normalized to range between 0.0 - 1.0\n",
    "\t\"\"\"\n",
    "\tflags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "\turl = str(url)\n",
    "\tresp = urllib.request.urlopen(url)\n",
    "\ttry:\n",
    "\t\tim = np.asarray(bytearray(resp.read()))\n",
    "\t\tim = cv2.imdecode(im, flags).astype(np.float32)/255\n",
    "\t\tif im is None: raise OSError(f'File from url not recognized by opencv: {url}')\n",
    "\t\treturn im\n",
    "\texcept Exception as e:\n",
    "\t\traise OSError(f'Error handling image from url at: {url}') from e\n",
    "\n",
    "\n",
    "def open_image(path):\n",
    "\t\"\"\" Opens an image using OpenCV given the file path.\n",
    "\t:param path: the file path of the image\n",
    "\t:return: the image in RGB format as numpy array of floats normalized to range between 0.0 - 1.0\n",
    "\t\"\"\"\n",
    "\tflags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "\tpath = str(path)\n",
    "\tif not os.path.exists(path):\n",
    "\t\traise OSError(f'No such file or directory: {path}')\n",
    "\telif os.path.isdir(path):\n",
    "\t\traise OSError(f'Is a directory: {path}')\n",
    "\telse:\n",
    "\t\ttry:\n",
    "\t\t\tim = cv2.imread(str(path), flags).astype(np.float32)/255\n",
    "\t\t\tif im is None: raise OSError(f'File not recognized by opencv: {path}')\n",
    "\t\t\treturn cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise OSError(f'Error handling image at: {path}') from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'pytorch-serverless-shun'\n",
    "STATE_DICT_NAME = 'pascal_1.h5'\n",
    "SZ = 224\n",
    "IMAGE_STATS = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "LABELS_PATH = 'lib/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.models'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-24e33233d96b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdensenet121\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet161\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet169\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensenet201\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnext_50_32x4d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnext_50_32x4d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnext_101_32x4d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnext_101_32x4d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnext_101_64x4d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnext_101_64x4d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.models'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "import torch, torchvision\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import kaiming_normal\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "from torchvision.models import vgg16_bn, vgg19_bn\n",
    "from torchvision.models import densenet121, densenet161, densenet169, densenet201\n",
    "\n",
    "from .models.resnext_50_32x4d import resnext_50_32x4d\n",
    "from .models.resnext_101_32x4d import resnext_101_32x4d\n",
    "from .models.resnext_101_64x4d import resnext_101_64x4d\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', message='Implicit dimension choice', category=UserWarning)\n",
    "\n",
    "\n",
    "def children(m): return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "\n",
    "\n",
    "def load_model(m, p): m.load_state_dict(torch.load(p, map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "def load_pre(pre, f, fn):\n",
    "\tm = f()\n",
    "\tpath = os.path.dirname(__file__)\n",
    "\tif pre: load_model(m, f'{path}/weights/{fn}.pth')\n",
    "\treturn m\n",
    "\n",
    "\n",
    "def _fastai_model(name, paper_title, paper_href):\n",
    "\tdef add_docs_wrapper(f):\n",
    "\t\tf.__doc__ = f\"\"\"{name} model from\n",
    "        `\"{paper_title}\" <{paper_href}>`_\n",
    "\n",
    "        Args:\n",
    "           pre (bool): If True, returns a model pre-trained on ImageNet\n",
    "        \"\"\"\n",
    "\t\treturn f\n",
    "\n",
    "\treturn add_docs_wrapper\n",
    "\n",
    "\n",
    "@_fastai_model('ResNeXt 50', 'Aggregated Residual Transformations for Deep Neural Networks',\n",
    "               'https://arxiv.org/abs/1611.05431')\n",
    "def resnext50(pre): return load_pre(pre, resnext_50_32x4d, 'resnext_50_32x4d')\n",
    "\n",
    "\n",
    "@_fastai_model('ResNeXt 101_32', 'Aggregated Residual Transformations for Deep Neural Networks',\n",
    "               'https://arxiv.org/abs/1611.05431')\n",
    "def resnext101(pre): return load_pre(pre, resnext_101_32x4d, 'resnext_101_32x4d')\n",
    "\n",
    "\n",
    "@_fastai_model('ResNeXt 101_64', 'Aggregated Residual Transformations for Deep Neural Networks',\n",
    "               'https://arxiv.org/abs/1611.05431')\n",
    "def resnext101_64(pre): return load_pre(pre, resnext_101_64x4d, 'resnext_101_64x4d')\n",
    "\n",
    "\n",
    "@_fastai_model('Densenet-121', 'Densely Connected Convolutional Networks',\n",
    "               'https://arxiv.org/pdf/1608.06993.pdf')\n",
    "def dn121(pre): return children(densenet121(pre))[0]\n",
    "\n",
    "\n",
    "@_fastai_model('Densenet-169', 'Densely Connected Convolutional Networks',\n",
    "               'https://arxiv.org/pdf/1608.06993.pdf')\n",
    "def dn161(pre): return children(densenet161(pre))[0]\n",
    "\n",
    "\n",
    "@_fastai_model('Densenet-161', 'Densely Connected Convolutional Networks',\n",
    "               'https://arxiv.org/pdf/1608.06993.pdf')\n",
    "def dn169(pre): return children(densenet169(pre))[0]\n",
    "\n",
    "\n",
    "@_fastai_model('Densenet-201', 'Densely Connected Convolutional Networks',\n",
    "               'https://arxiv.org/pdf/1608.06993.pdf')\n",
    "def dn201(pre): return children(densenet201(pre))[0]\n",
    "\n",
    "\n",
    "@_fastai_model('Vgg-16 with batch norm added', 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
    "               'https://arxiv.org/pdf/1409.1556.pdf')\n",
    "def vgg16(pre): return children(vgg16_bn(pre))[0]\n",
    "\n",
    "\n",
    "@_fastai_model('Vgg-19 with batch norm added', 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
    "               'https://arxiv.org/pdf/1409.1556.pdf')\n",
    "def vgg19(pre): return children(vgg19_bn(pre))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.torch_imports'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-eaddae97a4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_imports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcut_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcut\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.torch_imports'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "from .torch_imports import children\n",
    "\n",
    "\n",
    "def cut_model(m, cut):\n",
    "\treturn list(m.children())[:cut] if cut else [m]\n",
    "\n",
    "\n",
    "def num_features(m):\n",
    "\tc = children(m)\n",
    "\tif len(c) == 0: return None\n",
    "\tfor l in reversed(c):\n",
    "\t\tif hasattr(l, 'num_features'): return l.num_features\n",
    "\t\tres = num_features(l)\n",
    "\t\tif res is not None: return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cut_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-88bbc2bd3b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSetupModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LABELS_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-88bbc2bd3b57>\u001b[0m in \u001b[0;36mSetupModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSetupModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LABELS_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bad8341c4314>\u001b[0m in \u001b[0;36mclassification_model\u001b[0;34m(arch, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassification_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_multi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvnetBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1374a522b0c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, c, is_multi, is_reg, ps, xtra_fc, xtra_cut, custom_head, pretrained)\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_cut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mcut\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mxtra_cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcut_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_features\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcustom_head\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mAdaptiveConcatPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cut_model' is not defined"
     ]
    }
   ],
   "source": [
    "import os, json, traceback\n",
    "import urllib.parse\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "from fastai.core import A, T, VV_\n",
    "from fastai.transforms import tfms_from_stats\n",
    "\n",
    "\n",
    "STATS = A(IMAGE_STATS)\n",
    "TFMS = tfms_from_stats(STATS, SZ)[-1]\n",
    "\n",
    "\n",
    "class SetupModel(object):\n",
    "\tmodel = classification_model()\n",
    "\tlabels = get_labels(os.environ['LABELS_PATH'])\n",
    "\n",
    "\tdef __init__(self, f):\n",
    "\t\tself.f = f\n",
    "\t\tfile_path = f'/tmp/{STATE_DICT_NAME}'\n",
    "\t\tdownload_file(BUCKET_NAME, STATE_DICT_NAME, file_path)\n",
    "\t\tstate_dict = torch.load(file_path, map_location=lambda storage, loc: storage)\n",
    "\t\tself.model.load_state_dict(state_dict), self.model.eval()\n",
    "\t\tos.remove(file_path)\n",
    "\n",
    "\tdef __call__(self, *args, **kwargs):\n",
    "\t\treturn self.f(*args, **kwargs)\n",
    "\n",
    "\n",
    "def build_pred(label_idx, bb):\n",
    "\tlabel = SetupModel.labels[label_idx]\n",
    "\treturn dict(label=label, bb=bb)\n",
    "\n",
    "\n",
    "def parse_params(params):\n",
    "\timage_url = urllib.parse.unquote_plus(params.get('image_url', ''))\n",
    "\treturn dict(image_url=image_url)\n",
    "\n",
    "\n",
    "def predict(img):\n",
    "\tbatch = [T(TFMS(img))]\n",
    "\tinp = VV_(torch.stack(batch))\n",
    "\treturn SetupModel.model(inp).mean(0)\n",
    "\n",
    "def bb_to_list(bb):\n",
    "\tbb_list=[]\n",
    "\tfor b in bb:\n",
    "\t\tbb_list.append(b.item())\n",
    "\treturn bb_list\n",
    "\n",
    "\n",
    "@SetupModel\n",
    "def handler(event, _):\n",
    "\tif event is None: event = {}\n",
    "\tprint(event)\n",
    "\ttry:\n",
    "\t\t# keep the lambda function warm\n",
    "\t\tif event.get('detail-type') is 'Scheduled Event':\n",
    "\t\t\treturn 'nice & warm'\n",
    "\n",
    "\t\tparams = parse_params(event.get('queryStringParameters', {}))\n",
    "\t\tout = predict(open_image_url(params['image_url']))\n",
    "\n",
    "\t\tout = out.data.numpy\n",
    "\t\tbb = expit(out[:4])*224\n",
    "\t\tbb = bb_to_list(bb)\n",
    "\t\tc = out[4:]\n",
    "\t\tc = np.argmax(c)\n",
    "\n",
    "\t\tpreds = [build_pred(c, bb)]\n",
    "\n",
    "\t\tresponse_body = dict(predictions=preds)\n",
    "\t\tresponse = dict(statusCode=200, body=response_body)\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tresponse_body = dict(error=str(e), traceback=traceback.format_exc())\n",
    "\t\tresponse = dict(statusCode=500, body=response_body)\n",
    "\n",
    "\tresponse['body'] = json.dumps(response['body'])\n",
    "\tprint(response)\n",
    "\treturn response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
